{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# for running this it should be on the root of the project\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/maruanottoni/home/master/research/graph-corr-embedd/src')\n",
    "\n",
    "from src.models.SAE import StackedSparseAutoencoder\n",
    "from src.utils.conn_data import save_pickle\n",
    "from src.utils.parsers import str_2_bool\n",
    "from src.data.Simulation1Loader import Simulation1Loader\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stats.Stats import Stats\n",
    "class StackedSparseAutoencoder(nn.Module, Stats):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_sizes: list,\n",
    "                 bias: bool = True,\n",
    "                 dropout: float = 0.0,\n",
    "                 sparsity_penalty: float = 1e-4):\n",
    "        super(StackedSparseAutoencoder, self).__init__()\n",
    "\n",
    "        # parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.sparsity_penalty = sparsity_penalty\n",
    "\n",
    "        # encoder\n",
    "        encoder_layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            encoder_layers.append(nn.Linear(prev_size, hidden_size, bias=bias))\n",
    "            encoder_layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout > 0:\n",
    "                encoder_layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # decoder\n",
    "        decoder_layers = []\n",
    "        hidden_sizes.reverse()  # reverse the hidden sizes for symmetric decoder\n",
    "        for hidden_size in hidden_sizes[:-1]:\n",
    "            decoder_layers.append(nn.Linear(prev_size, hidden_size, bias=bias))\n",
    "            decoder_layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout > 0:\n",
    "                decoder_layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        # add the last layer without ReLU to reconstruct the input\n",
    "        decoder_layers.append(nn.Linear(prev_size, input_size, bias=bias))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, z\n",
    "\n",
    "    def loss_function(self, recon_x, x):\n",
    "        # Mean squared error for reconstruction loss\n",
    "        recon_loss = F.mse_loss(recon_x, x)\n",
    "\n",
    "        # L1 loss for sparsity penalty\n",
    "        sparsity_loss = 0\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                sparsity_loss += torch.sum(torch.abs(layer.weight))\n",
    "\n",
    "        total_loss = recon_loss + self.sparsity_penalty * sparsity_loss\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  7.57it/s]\n",
      "/Users/maruanottoni/miniforge3/envs/gce/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'simulation1'\n",
    "sample = True\n",
    "batch_size = 1\n",
    "\n",
    "# define dataset\n",
    "sim = Simulation1Loader(name=dataset_name, sample=sample)\n",
    "loader = sim.create_graph_loader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"sae\"\n",
    "input_size = 100\n",
    "hidden_sizes = [50, 25, 50]\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "sparsity_penalty = 1e-4\n",
    "\n",
    "# define model\n",
    "model1    = StackedSparseAutoencoder(input_size=input_size,\n",
    "                                    hidden_sizes=hidden_sizes,\n",
    "                                    dropout=dropout,\n",
    "                                    sparsity_penalty=sparsity_penalty)\n",
    "\n",
    "model2    = StackedSparseAutoencoder(input_size=input_size,\n",
    "                                    hidden_sizes=hidden_sizes,\n",
    "                                    dropout=dropout,\n",
    "                                    sparsity_penalty=sparsity_penalty)\n",
    "\n",
    "\n",
    "\n",
    "# define optimizer\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99, Train Loss I & II: 2.1495 & 2.1057: 100%|██████████| 100/100 [00:06<00:00, 14.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize tqdm\n",
    "pbar = tqdm(range(epochs))\n",
    "train_pred, train_true = [], []\n",
    "xs_train, zs_train = [], []\n",
    "epoch_loss_train = []\n",
    "for epoch in pbar:\n",
    "    epoch_loss1, epoch_loss2 = 0, 0 \n",
    "    for data in loader:\n",
    "        # get inputs\n",
    "        x1 = data.x[0, :, :]\n",
    "        x2 = data.x[1, :, :]\n",
    "\n",
    "        # forward pass\n",
    "        x1_hat, z1, = model1.forward(x1)\n",
    "        x2_hat, z2  = model2.forward(x1)\n",
    "\n",
    "        # compute correlation between embeddings (true target)\n",
    "        corr = model1.compute_spearman_rank_correlation(x=z1.flatten().detach(), y=z2.flatten().detach())\n",
    "\n",
    "        # compute loss\n",
    "        loss1 = model1.loss_function(x1_hat, x1)\n",
    "        loss2 = model2.loss_function(x2_hat, x2)\n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        epoch_loss1 += loss1.item()\n",
    "        epoch_loss2 += loss2.item()\n",
    "\n",
    "    # update tqdm\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(\"Train Epoch: %d, Train Loss I & II: %.4f & %.4f\" % (epoch, epoch_loss1, epoch_loss2))\n",
    "\n",
    "    # save loss\n",
    "    epoch_loss_train.append([epoch_loss1, epoch_loss2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/zy_pbb_s3591fwh2_q7z6g0h0000gn/T/ipykernel_68117/1427370118.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_pred = torch.tensor(train_pred)\n",
      "/var/folders/ns/zy_pbb_s3591fwh2_q7z6g0h0000gn/T/ipykernel_68117/1427370118.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_true = torch.tensor(train_true)\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 163.75it/s]\n"
     ]
    }
   ],
   "source": [
    " # pred list to tensor\n",
    "train_pred = torch.tensor(train_pred)\n",
    "train_true = torch.tensor(train_true)\n",
    "\n",
    "pbar = tqdm(enumerate(loader), total=len(loader))\n",
    "test_pred = []\n",
    "test_true = []\n",
    "with torch.no_grad():\n",
    "    for s, data in pbar:\n",
    "        # get inputs\n",
    "        x1 = data.x[0, :, :]\n",
    "        x2 = data.x[1, :, :]\n",
    "\n",
    "        # forward pass\n",
    "        x1_hat, z1 = model1.forward(x1)\n",
    "        x2_hat, z2 = model2.forward(x2)\n",
    "\n",
    "        # compute correlation between embeddings (true target)\n",
    "        corr = model1.compute_spearman_rank_correlation(x=z1.flatten().detach(), y=z2.flatten().detach())\n",
    "\n",
    "        # store pred and true values\n",
    "        test_pred.append(corr)\n",
    "        test_true.append(data.y)\n",
    "\n",
    "        # update tqdm\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Test Sample: {s}\")\n",
    "    \n",
    "# pred list to tensor\n",
    "test_pred = torch.tensor(test_pred)\n",
    "test_true = torch.tensor(test_true)\n",
    "\n",
    "results = {\n",
    "    \"train_pred\": train_pred,\n",
    "    \"train_true\": train_true,\n",
    "    \"test_pred\": test_pred,\n",
    "    \"test_true\": test_true,\n",
    "    \"epoch_loss_train\": epoch_loss_train,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = f'{args.model_name}_{int(args.n_hidden)}_{int(args.n_layers_enc)}_{int(args.n_layers_dec)}'\n",
    "# check if file exists\n",
    "#output_path = f\"{os.path.dirname(__file__)}/data/outputs/{args.dataset_name}/{model_name}\"\n",
    "#if not os.path.exists(output_path):\n",
    "#    os.makedirs(output_path)\n",
    "\n",
    "# save file\n",
    "#if args.sample:\n",
    "#    save_pickle(path=f\"{output_path}/sample_results.pkl\", obj=results)\n",
    "#else:\n",
    "#    save_pickle(path=f\"{output_path}/results.pkl\", obj=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.5100421931964476\n",
      "RMSE: 0.580110447828865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "mae = mean_absolute_error(test_true, test_pred)\n",
    "rmse = sqrt(mean_squared_error(test_true, test_pred))\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple times same architecture:w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99, Train Loss I & II: 2.2518 & 2.1861: 100%|██████████| 100/100 [00:09<00:00, 10.58it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 174.98it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2535 & 2.1922: 100%|██████████| 100/100 [00:08<00:00, 12.23it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 189.23it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2386 & 2.1919: 100%|██████████| 100/100 [00:06<00:00, 15.35it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 148.72it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2525 & 2.1982: 100%|██████████| 100/100 [00:07<00:00, 13.56it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 241.08it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2266 & 2.1689: 100%|██████████| 100/100 [00:06<00:00, 15.24it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 90.65it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name =\"sae\"\n",
    "input_size = 100\n",
    "hidden_sizes = [50, 25, 50]\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "sparsity_penalty = 1e-4\n",
    "n_runs = 5\n",
    "\n",
    "results_list = []\n",
    "for i in range(n_runs):\n",
    "    # define model\n",
    "    model1    = StackedSparseAutoencoder(input_size=input_size,\n",
    "                                        hidden_sizes=hidden_sizes,\n",
    "                                        dropout=dropout,\n",
    "                                        sparsity_penalty=sparsity_penalty)\n",
    "\n",
    "    model2    = StackedSparseAutoencoder(input_size=input_size,\n",
    "                                        hidden_sizes=hidden_sizes,\n",
    "                                        dropout=dropout,\n",
    "                                        sparsity_penalty=sparsity_penalty)\n",
    "\n",
    "\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer1 = optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "    optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "    # initialize tqdm\n",
    "    pbar = tqdm(range(epochs))\n",
    "    train_pred, train_true = [], []\n",
    "    xs_train, zs_train = [], []\n",
    "    epoch_loss_train = []\n",
    "    for epoch in pbar:\n",
    "        epoch_loss1, epoch_loss2 = 0, 0 \n",
    "        for data in loader:\n",
    "            # get inputs\n",
    "            x1 = data.x[0, :, :]\n",
    "            x2 = data.x[1, :, :]\n",
    "\n",
    "            # forward pass\n",
    "            x1_hat, z1, = model1.forward(x1)\n",
    "            x2_hat, z2  = model2.forward(x1)\n",
    "\n",
    "            # compute correlation between embeddings (true target)\n",
    "            corr = model1.compute_spearman_rank_correlation(x=z1.flatten().detach(), y=z2.flatten().detach())\n",
    "\n",
    "            # compute loss\n",
    "            loss1 = model1.loss_function(x1_hat, x1)\n",
    "            loss2 = model2.loss_function(x2_hat, x2)\n",
    "\n",
    "            # backward and optimize\n",
    "            optimizer1.zero_grad()\n",
    "            loss1.backward()\n",
    "            optimizer1.step()\n",
    "\n",
    "            optimizer2.zero_grad()\n",
    "            loss2.backward()\n",
    "            optimizer2.step()\n",
    "\n",
    "            epoch_loss1 += loss1.item()\n",
    "            epoch_loss2 += loss2.item()\n",
    "\n",
    "        # update tqdm\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\"Train Epoch: %d, Train Loss I & II: %.4f & %.4f\" % (epoch, epoch_loss1, epoch_loss2))\n",
    "\n",
    "\n",
    "    # pred list to tensor\n",
    "    train_pred = torch.tensor(train_pred)\n",
    "    train_true = torch.tensor(train_true)\n",
    "\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader))\n",
    "    test_pred = []\n",
    "    test_true = []\n",
    "    with torch.no_grad():\n",
    "        for s, data in pbar:\n",
    "            # get inputs\n",
    "            x1 = data.x[0, :, :]\n",
    "            x2 = data.x[1, :, :]\n",
    "\n",
    "            # forward pass\n",
    "            x1_hat, z1 = model1.forward(x1)\n",
    "            x2_hat, z2 = model2.forward(x2)\n",
    "\n",
    "            # compute correlation between embeddings (true target)\n",
    "            corr = model1.compute_spearman_rank_correlation(x=z1.flatten().detach(), y=z2.flatten().detach())\n",
    "\n",
    "            # store pred and true values\n",
    "            test_pred.append(corr)\n",
    "            test_true.append(data.y)\n",
    "\n",
    "            # update tqdm\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"Test Sample: {s}\")\n",
    "\n",
    "        # save loss\n",
    "        epoch_loss_train.append([epoch_loss1, epoch_loss2])\n",
    "        \n",
    "\n",
    "    # pred list to tensor\n",
    "    test_pred = torch.tensor(test_pred)\n",
    "    test_true = torch.tensor(test_true)\n",
    "\n",
    "    results = {\n",
    "        \"train_pred\": train_pred,\n",
    "        \"train_true\": train_true,\n",
    "        \"test_pred\": test_pred,\n",
    "        \"test_true\": test_true,\n",
    "        \"epoch_loss_train\": epoch_loss_train,\n",
    "    }\n",
    "    results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_list = []\n",
    "rmse_list = []\n",
    "for i in range(len(results_list)):\n",
    "    test_true = results_list[i]['test_true']\n",
    "    test_pred = results_list[i]['test_pred']\n",
    "\n",
    "    mae = mean_absolute_error(test_true, test_pred)\n",
    "    rmse = sqrt(mean_squared_error(test_true, test_pred))\n",
    "\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5695954334114186, 0.03552441607647186)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mae_list), np.std(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6397551084884349, 0.03139197371433021)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rmse_list), np.std(rmse_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple times different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99, Train Loss I & II: 2.2396 & 2.1992: 100%|██████████| 100/100 [00:09<00:00, 10.44it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 203.46it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2257 & 2.1869: 100%|██████████| 100/100 [00:09<00:00, 10.80it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 141.93it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2223 & 2.1606: 100%|██████████| 100/100 [00:07<00:00, 13.35it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 111.23it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2422 & 2.1705: 100%|██████████| 100/100 [00:07<00:00, 13.63it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 201.30it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2242 & 2.1915: 100%|██████████| 100/100 [00:09<00:00, 10.34it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 246.12it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.1785 & 2.2011: 100%|██████████| 100/100 [00:27<00:00,  3.58it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 89.66it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.1942 & 2.1437: 100%|██████████| 100/100 [00:24<00:00,  4.17it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 78.88it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.1987 & 2.1564: 100%|██████████| 100/100 [00:20<00:00,  4.97it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 53.90it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.1952 & 2.1557: 100%|██████████| 100/100 [00:23<00:00,  4.20it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 116.37it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2118 & 2.1505: 100%|██████████| 100/100 [00:26<00:00,  3.78it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 68.23it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2193 & 2.1679: 100%|██████████| 100/100 [00:20<00:00,  4.84it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 65.35it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2535 & 2.1645: 100%|██████████| 100/100 [00:18<00:00,  5.38it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 60.88it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2255 & 2.1586: 100%|██████████| 100/100 [00:19<00:00,  5.12it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 121.76it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2346 & 2.1828: 100%|██████████| 100/100 [00:21<00:00,  4.64it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 52.18it/s]\n",
      "Train Epoch: 99, Train Loss I & II: 2.2587 & 2.1924: 100%|██████████| 100/100 [00:31<00:00,  3.19it/s]\n",
      "Test Sample: 9: 100%|██████████| 10/10 [00:00<00:00, 54.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameter configurations to test\n",
    "configurations = [\n",
    "    {\"hidden_sizes\": [50, 25, 50], \"dropout\": 0.5},\n",
    "    {\"hidden_sizes\": [100, 50, 100], \"dropout\": 0.4},\n",
    "    {\"hidden_sizes\": [75, 35, 75], \"dropout\": 0.6},\n",
    "]\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for config in configurations:\n",
    "    for run in range(n_runs):\n",
    "        hidden_sizes = config[\"hidden_sizes\"]\n",
    "        dropout = config[\"dropout\"]\n",
    "\n",
    "        # Initialize models with current configuration\n",
    "        model1 = StackedSparseAutoencoder(input_size=input_size,\n",
    "                                        hidden_sizes=hidden_sizes,\n",
    "                                        dropout=dropout,\n",
    "                                        sparsity_penalty=sparsity_penalty)\n",
    "\n",
    "        model2 = StackedSparseAutoencoder(input_size=input_size,\n",
    "                                        hidden_sizes=hidden_sizes,\n",
    "                                        dropout=dropout,\n",
    "                                        sparsity_penalty=sparsity_penalty)\n",
    "\n",
    "        # Define optimizers for each model\n",
    "        optimizer1 = optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "        optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "        # initialize tqdm\n",
    "        pbar = tqdm(range(epochs))\n",
    "        train_pred, train_true = [], []\n",
    "        xs_train, zs_train = [], []\n",
    "        epoch_loss_train = []\n",
    "        for epoch in pbar:\n",
    "            epoch_loss1, epoch_loss2 = 0, 0 \n",
    "            for data in loader:\n",
    "                # get inputs\n",
    "                x1 = data.x[0, :, :]\n",
    "                x2 = data.x[1, :, :]\n",
    "\n",
    "                # forward pass\n",
    "                x1_hat, z1, = model1.forward(x1)\n",
    "                x2_hat, z2  = model2.forward(x1)\n",
    "\n",
    "                # compute correlation between embeddings (true target)\n",
    "                corr = model1.compute_spearman_rank_correlation(x=z1.flatten().detach(), y=z2.flatten().detach())\n",
    "\n",
    "                # compute loss\n",
    "                loss1 = model1.loss_function(x1_hat, x1)\n",
    "                loss2 = model2.loss_function(x2_hat, x2)\n",
    "\n",
    "                # backward and optimize\n",
    "                optimizer1.zero_grad()\n",
    "                loss1.backward()\n",
    "                optimizer1.step()\n",
    "\n",
    "                optimizer2.zero_grad()\n",
    "                loss2.backward()\n",
    "                optimizer2.step()\n",
    "\n",
    "                epoch_loss1 += loss1.item()\n",
    "                epoch_loss2 += loss2.item()\n",
    "\n",
    "            # update tqdm\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(\"Train Epoch: %d, Train Loss I & II: %.4f & %.4f\" % (epoch, epoch_loss1, epoch_loss2))\n",
    "\n",
    "\n",
    "        # pred list to tensor\n",
    "        train_pred = torch.tensor(train_pred)\n",
    "        train_true = torch.tensor(train_true)\n",
    "\n",
    "        pbar = tqdm(enumerate(loader), total=len(loader))\n",
    "        test_pred = []\n",
    "        test_true = []\n",
    "        with torch.no_grad():\n",
    "            for s, data in pbar:\n",
    "                # get inputs\n",
    "                x1 = data.x[0, :, :]\n",
    "                x2 = data.x[1, :, :]\n",
    "\n",
    "                # forward pass\n",
    "                x1_hat, z1 = model1.forward(x1)\n",
    "                x2_hat, z2 = model2.forward(x2)\n",
    "\n",
    "                # compute correlation between embeddings (true target)\n",
    "                corr = model1.compute_spearman_rank_correlation(x=z1.flatten().detach(), y=z2.flatten().detach())\n",
    "\n",
    "                # store pred and true values\n",
    "                test_pred.append(corr)\n",
    "                test_true.append(data.y)\n",
    "\n",
    "                # update tqdm\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(f\"Test Sample: {s}\")\n",
    "\n",
    "            # save loss\n",
    "            epoch_loss_train.append([epoch_loss1, epoch_loss2])\n",
    "            \n",
    "\n",
    "        # pred list to tensor\n",
    "        test_pred = torch.tensor(test_pred)\n",
    "        test_true = torch.tensor(test_true)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        # After training and evaluation, compile results for this configuration\n",
    "        results = {\n",
    "            \"config\": config,\n",
    "            \"run\": run,\n",
    "            \"train_pred\": train_pred,\n",
    "            \"train_true\": train_true,\n",
    "            \"test_pred\": test_pred,\n",
    "            \"test_true\": test_true,\n",
    "            \"epoch_loss_train\": epoch_loss_train,\n",
    "        }\n",
    "        results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_list = []\n",
    "rmse_list = []\n",
    "    for i in range(len(results_list)):\n",
    "        test_true = results_list[i]['test_true']\n",
    "        test_pred = results_list[i]['test_pred']\n",
    "\n",
    "        mae = mean_absolute_error(test_true, test_pred)\n",
    "        rmse = sqrt(mean_squared_error(test_true, test_pred))\n",
    "        mae_list.append(mae)\n",
    "        rmse_list.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}</th>\n",
       "      <td>0.584968</td>\n",
       "      <td>0.653801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}</th>\n",
       "      <td>0.449809</td>\n",
       "      <td>0.531727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}</th>\n",
       "      <td>0.623423</td>\n",
       "      <td>0.688414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}</th>\n",
       "      <td>0.568513</td>\n",
       "      <td>0.635418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}</th>\n",
       "      <td>0.592732</td>\n",
       "      <td>0.654140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}</th>\n",
       "      <td>0.530975</td>\n",
       "      <td>0.602576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}</th>\n",
       "      <td>0.512614</td>\n",
       "      <td>0.585895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}</th>\n",
       "      <td>0.600431</td>\n",
       "      <td>0.663290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}</th>\n",
       "      <td>0.535248</td>\n",
       "      <td>0.607410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}</th>\n",
       "      <td>0.543774</td>\n",
       "      <td>0.616702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}</th>\n",
       "      <td>0.549675</td>\n",
       "      <td>0.618942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}</th>\n",
       "      <td>0.568413</td>\n",
       "      <td>0.634477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}</th>\n",
       "      <td>0.568787</td>\n",
       "      <td>0.638541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}</th>\n",
       "      <td>0.554142</td>\n",
       "      <td>0.624540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}</th>\n",
       "      <td>0.504067</td>\n",
       "      <td>0.579186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       MAE      RMSE\n",
       "{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}    0.584968  0.653801\n",
       "{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}    0.449809  0.531727\n",
       "{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}    0.623423  0.688414\n",
       "{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}    0.568513  0.635418\n",
       "{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}    0.592732  0.654140\n",
       "{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}  0.530975  0.602576\n",
       "{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}  0.512614  0.585895\n",
       "{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}  0.600431  0.663290\n",
       "{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}  0.535248  0.607410\n",
       "{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}  0.543774  0.616702\n",
       "{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}    0.549675  0.618942\n",
       "{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}    0.568413  0.634477\n",
       "{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}    0.568787  0.638541\n",
       "{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}    0.554142  0.624540\n",
       "{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}    0.504067  0.579186"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Initialize lists for MAE and RMSE\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "configs = [results_list[i]['config'] for i in range(len(results_list))]\n",
    "\n",
    "# Loop over the results_list\n",
    "for result in results_list:\n",
    "    # Calculate MAE and RMSE\n",
    "    mae = mean_absolute_error(result['test_true'], result['test_pred'])\n",
    "    rmse = sqrt(mean_squared_error(result['test_true'], result['test_pred']))\n",
    "    \n",
    "    # Append MAE and RMSE to the respective lists\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'MAE': mae_list,\n",
    "    'RMSE': rmse_list\n",
    "}, index=configs)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">MAE</th>\n",
       "      <th colspan=\"2\" halign=\"left\">RMSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}</th>\n",
       "      <td>0.544608</td>\n",
       "      <td>0.033219</td>\n",
       "      <td>0.615175</td>\n",
       "      <td>0.029128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}</th>\n",
       "      <td>0.563889</td>\n",
       "      <td>0.066814</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>0.059607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}</th>\n",
       "      <td>0.549017</td>\n",
       "      <td>0.026525</td>\n",
       "      <td>0.619137</td>\n",
       "      <td>0.023649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       MAE            \\\n",
       "                                                      mean       std   \n",
       "{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}  0.544608  0.033219   \n",
       "{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}    0.563889  0.066814   \n",
       "{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}    0.549017  0.026525   \n",
       "\n",
       "                                                      RMSE            \n",
       "                                                      mean       std  \n",
       "{'hidden_sizes': [100, 50, 100], 'dropout': 0.4}  0.615175  0.029128  \n",
       "{'hidden_sizes': [50, 25, 50], 'dropout': 0.5}    0.632700  0.059607  \n",
       "{'hidden_sizes': [75, 35, 75], 'dropout': 0.6}    0.619137  0.023649  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index = df.index.map(str)\n",
    "grouped_df = df.groupby(df.index).agg(['mean', 'std'])\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
